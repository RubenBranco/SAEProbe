# SAEProbe

My implementation of Sparse Autoencoders for personal use. Since I work with networks that are not LLMs, and whose architectures can vary a lot, the code is as general as possible.

These implementations are by no means the most efficient, nor the most elegant code, and probably missing a lot of useful features (and SAE architectures) present in other packages. They could also contain errors / bugs.

Yet, if it is useful for you, the code is publicly available.

For feature-rich packages, or nice implementations, albeit tailored for LLMs, you may find useful to visit the following:

* [SAELens](https://github.com/jbloomAus/SAELens/tree/main)
* [EleutherAI SAE](https://github.com/EleutherAI/sae/tree/main)
* [PaulPaul's Project](https://github.com/PaulPauls/llama3_interpretability_sae)
* [TransformerLens](https://github.com/neelnanda-io/TransformerLens)
